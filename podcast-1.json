{"podcast_details": {"podcast_title": "Naked Data Science", "episode_title": "Central Limit Theorem in Plain English", "episode_image": "https://storage.buzzsprout.com/variants/2futmk4bed897wtddw8t44cmixzd/f6fd9b4ca4e468e7e11c8350463c894b252ee834d352e0a8d889b97ac83aedef.jpg", "episode_transcript": " Hello everyone, welcome back to another episode of Naked Data Science. Now we are having a new format in this episode. So Nima gives me a topic, which is central limit theorem. And then I spend an hour learning about it. Then we have a little chat. Now you will hear about why we are doing this format in the episode. But if you like it, and you want to hear more episodes like this, please email us at hello at nds.show. That will really help us decide if we are going to make more episodes like this. Thank you and enjoy this episode. Hello everyone, welcome to another episode of Naked Data Science. This is how. Nima. So I'm quite excited about today's episode because we are trying something new that we have never done before on the show. Yeah, let's see how it goes. Yeah, so we're talking about this last week as we finished recording the previous episode. We realized that there's a challenge we commonly see in a lot of the students we work with and also in our previous colleagues or current colleagues that work in data science, which is a lot of times people would come across a topic. It might be some, let's say, quote unquote, basic topics, and they feel that they want to understand it a little bit more. They spend like an hour, two hours looking into it, but usually have difficulty getting it through. I think it's quite common and it's not a matter of spending the time most of the time. A lot of times it could be selecting the right topic as well as digging through all the layers of abstraction, all that builds up to that final understanding. And I guess sometimes it's also not getting bogged down by all the details that make something correct. I think that is a quite common, but also quite valuable thing if we can help people with, because a lot of times in data science, it's good to have a good working mental model of some basic concepts or fundamental concepts. If you know that, then it really helps you, for example, when you apply the more advanced techniques based on that, or when you see a situation that is not familiar or you see a situation that doesn't seem to make sense. But if you think it through the more fundamental concepts, then they usually help you to progress further. That's why we say, okay, let's try to talk about one of such a topic for one episode and let's make it realistic. So the way we come out with is that we say, okay, most of the time people should have probably about half an hour, an hour, maximum two hours to look into something, right? Everyone is busy alongside their job and daily life. So then we kind of pose a challenge for myself, which is I was going to spend one hour looking into a concept, in this case, central limit theorem. And then within this one hour, I'll look into whatever resources that I found useful. And by the end of this hour, I am going to summarize it, put it into a kind of application, and then maybe also come up with some questions that we can discuss here. Did I miss anything, Nima? No, I think that I'm really excited to see what will happen when we do this exercise. And especially what is really appealing is what you can learn about the topic sometimes in just by just spending one or two hours in learning about it, kind of out of the blue, probably you've already been exposed to the concept, you've heard about it, but now you're going for a little deeper understanding. It's also very interesting to me that you went for taking this angle of what could be the applications of this thing, because that's the gap which is hard to fill at points where you can spend a lot of time understanding a topic. You can spend maybe more time in understanding why this is true, like mathematically, why does it work, or algorithmically, why is it the case? What are the intuitions there? But then when it comes to real life and applying these things, there's still a big problem to solve or a big skill to acquire. When is this applicable? Is this a situation where I can use this tool? So given all of this, I'm really looking forward to see how this pans out. Okay, sounds like pretty high expectations. But all this is given the time, but one hour. So let me share with you what I found out in that hour. Just a little bit background, this is not the first time I learned about central limit theorem. I remember it was in my first year freshman university, I think I did some basic statistic course and this was part of it. But then that's really the last time I came across it, like in a kind of dedicated way throughout my time working, but then I have not really looked into it. So when I look into this, the first thing I came across before I can build a mental model around that is another concept, which is the law of large numbers. It's a quite fundamental concept in statistics. So the way I understand it is that imagine that there is a random variable X, right? So this is kind of like the more mathematic term. You can think of it as, okay, you have some kind of a process that generates some kind of number, right? Let's call that X. And then imagine that it run a very, very long period of time and then it generate tons of numbers and then you can average that. And then the average you get out of it, imagine, you know, the beginning of time and the end of time, then what you get is a population mean of that process, right? So you've got the average of what that random variable is. And the law of large numbers says in real life, you cannot really observe the population mean because you don't have until the end of the time. But as long as you start running this process and the more times you run it, the more it will converge or it will approach the population mean if you calculate the mean of all those runs that you have. That I think practically in a lot of workplaces, people just think of that as, okay, the more times you run an experiment, for example, or the more number of people you expose, for example, A, B test two, then the more likely you are getting a mean that is close to the true mean of that changed process or things like that. So that is the law of large number. And do I miss anything? I think it was a good summary and love to add to that if you could is some more examples of those processes that generate something. Because in a way, we always go to this mathematical definition of a random variable. And in the end, to be correct, you have to go there. But I guess the concept of the random variable itself could be sometimes hard to grasp if you're not thinking in those probabilistic terms. So everybody uses dice. So I'm not going to use dice as an example. I think this is what people typically kind of overemphasize on the wrong thing. So whenever people talk about statistics or probability, then you see on the cover of the book, there's some dices, people draw some dices. But it can be just pretty much everything. So one example I would say would be imagine that you commute to work, right? You walk to the train station, you take a train, and then the train arrives at the station near your work. And then you get off the train and then you work to your office. Imagine you do this every day. This in and of itself is a random process, because every time, the exact time you spend going out of your home and arriving at your office, the time, the duration in between is different every time, right? And you can think of that as a random process or even part of that. So even how much time it takes you to work from your home to the station is not exactly the same every time, right? Because every day, the way you work is slightly different. Maybe the weather is slightly different, maybe on the way they're fixing some things where you have to take a detour. So all these kinds of things make this process a random process. So when you think like that, really, most of the stuff in real life is random process. Yeah, I really like that example. I think that's one of the best. I always come across these processes which happen daily and randomness there is basically the variability that is somehow out of our model in a way. It's a variability in what you want to measure. And I think even making it simpler, whenever we have variability in our measurements in a way, we could be looking at it as dealing with a random variable. So whenever this uncertainty is there. And I love this way of looking at things as a process, generating things. But sometimes it's even very natural processes that we can think about. So I don't know, scientists go to this new area, they find these new species, subspecies of rabbits, for instance, and they want to get an idea about how big these guys are. And they start measuring how much they weigh. They start measuring how long they are in length. And that is itself again, or can be looked at as a random variable or random process. So in that sense, the scores that students get in an exam could be thought about as a random process. Whatever kind of measurements that you want to summarize about the population can be thought of like that. And that's, I think, a key in seeing the relevance of central limit theorem or the law of large numbers as we are talking about it now, is that when we're dealing with a population of things that are, of course, generated by some process in the end, we always get these measurements about a sample of those populations. Of course, we are hoping that that sample is representative of the population. But then for me, at least, when I think about measurements like this, and these measurements could be averaging weights of the rabbits or how long these rabbits are in this specific area. But when thinking about it like that, then suddenly these two key fundamental theorems become immediately relevant and of course, somehow surprising in how applicable they are as we, I'm sure we're going to talk more about them. So kudos on looking at it with that kind of example and of course, you went for a very realistic, nice example of the process, but I just wanted to point out this process could be any kind of natural process, anything that lets us do some kind of measurements with variability. Yeah, there is something kind of deeply beautiful in how much these two things apply to the nature. Let's come to that later. Now, just following the example I gave before, imagine you commuting to work. Actually from there, we can go directly to the central limit theorem, which is building on top of the law of large numbers, which says that if you sum up a bunch of random variables generated through different processes and assume that these variables are independent. And then, so in our example, that will be the total time, total commute time, which is the sum of, let's say the working time from your home to the train station, the time for the train to take you to the station near the work and then the time it takes for you to work from that station to your office. Let's say those are the three variables and if you add them up, you have the total commute time and the thing is that if you do that again and again and again and again and again, then you will see that the total commute time turn to distribute like a normal distribution. It's never really a normal distribution, but then it usually approximates towards that, which just means that the more times you do this, the more you see it kind of like becoming a bell curve. It can be a little bit fat, it can be a little bit thin, but that's what you turn to see. So that's my understanding of the central limit theorem. In terms of the application, so as long as you have a bunch of processes that are the independent from each other and oh, I think it's important that none of these processes is really skilled, but then as long as that's not the case, then you can apply this to a wide range of things. So that's my understanding, my mental model of the central limit theorem. I think that's a good summary, definitely a good place to start. Maybe about the last point that you mentioned, none of these processes is highly skilled. I think that's a very interesting point to dig into later, maybe even broader looking into what kind of assumptions are behind the applicability of a central limit theorem. About that one point that came to my mind is that the way of thinking is typically that we have copies of the same random variable and you're basically dealing with identical copies, independent and identical copies of the same random variable. So basically means that whatever process that you assume generates one variable is also behind generating the other variable. One thing that gets lost a bit exactly related to this notion of independent copies of the same random variable and it's pointing out to the fact that when we go and talk about central limit theorem, we are talking about the sampling distribution and I think that's a very key point to bring into the picture. So what do we mean by the sampling distribution? Let's say you draw samples from some sort of population that is generated by this process that we talked about and then these samples themselves form a distribution. The shape of this distribution is not necessarily and does not have to be in any way the same as the shape of the original distribution that generated this data. At least in many cases, it's useful to think about the central limit theorem. So let's say this example, continue with this stupid examples of the rabbits that I got stuck with now. You go every month to that same location, gather a group of rabbits and start measuring their weights, for instance. When you look at data that you gathered each month, every time you gather the mean of this distribution and if you make a distribution of those mean, then this distribution in the limit is going to be looking like a normal distribution. And that is one of the beauties of this theorem in a way that given some conditions that I'm sure we will talk about as we go on, it really doesn't matter what your original distribution look like. The sampling distribution is guaranteed to look like a normal distribution. So you can have scootness in your original distribution. That is not necessarily a big problem. Even under those conditions, basically given that we know, for instance, that the mean and the standard deviation of the original distribution is well defined and of course some other small conditions, then you're guaranteed that your sampling distribution looks like a normal distribution. And I think that is one of the magical properties of the central limit theorem. Or maybe in a way, one of the beautiful things of how nature and mathematics could go hand in hand and give us this convenient way of thinking and reasoning about the whole population. But at least one good way to use central limit theorem is to think about the angle of the sampling distribution. Basically, if I were to take a lot of samples of this distribution, what would that distribution itself look like? Yeah. I think that is something that's a bit difficult to wrap your head around, especially the one the first time I came across this sample mean. So it's a bit tricky because we are not talking about you take one set of sample, what's the mean of that, but we are talking about we take many samples of n instances or n records, and then you kind of look at the distribution of those mean of those samples. And in that case, you start to see a normal distribution. Imagine not all distributions have to be normal for us to be able to apply it. And I think that's the part where kind of the wide applicability and beauty of this method comes in. Because I think technically central limit theorem is talking about sum of random variables, which when you divide them by the number of observations, you get the mean of random variables. So that's the key point when this transition happens. In a way, law of large numbers is always talking about that mean that you will observe, that sample mean that you will observe. And then the central limit theorem goes a step further and then talks about the distribution of those means that you will observe. And that's the reason where you can have a distribution which is not looking normal at all, but satisfy some required conditions. And then when you draw a sample from it, you know that the mean that you drive is going to be governed by a normal distribution if you were to repeat that measurement or that kind of experiment. So it's maybe a kind of meta level of talking about distribution. It's the distributions of your sample means. And it is difficult, I think, at least for me, it was a bit difficult to wrap my head around it the first times. But one way that makes it more concrete, or at least for me, more intuitive to see the problem from this angle is thinking about experiments and A-B testing. I think that's one of the more natural ways where you have immediately the reason to think in this term. Imagine they made a new drug, a new treatment, a new change tweak in the UI, whatever you want to experiment with. And then we are assuming that overall this treatment version might have a different mean than the control version for all the participants in this experiment. We do one experiment, especially if it's a medical experiment. It's quite sensitive. It could be quite tricky to even have, I don't know, 20, 30, definitely thousands of people in the experiment. Now we've drew a mean for the treatment. And of course we have a mean for the control. We have one mean. We've done one measurement. And now that's the part where it becomes important to see how representative this measurement could be. And that's the part where if you know that the means that you draw are coming from a normal distribution, then you can basically start reasoning about how likely it is that I got a sample that is this far from the true mean. So basically then you go through your typical reasoning about a normal distribution, the 65, 98, 99 rule. I can be one standard deviations and that's the probability of falling between one standard deviation, two standard deviation, and so on. So all of this beautiful theory about normal distributions suddenly becomes available to kind of backward reason about the true mean of the distribution that you don't have direct access to. You're always going to be able to draw some samples from. And that difference in the true mean of the pretreatment and post-treatment processes, for example, those things are usually very valuable for businesses, right? Because you can imagine that if on average the new version of your website convert just 0.1% more users than the old version for a lot of business that directly links to, I don't know, millions of euro or dollar improvements. So that's why these numbers are super important. Now we talk about mean a lot, right? What about skill? What about other kind of summary statistics, right? Let's say in some business situations, I not only care about difference in the mean, but I also want to make sure that the new version don't end up with more disastrous situation or more, let's say outlier situation than the control version. Does central limit theorem say anything about that or can be used for that? Good question. I actually don't think I know a direct connection between central limit theorem and outliers in that way. Only one kind of connection is the other way around that some distributions make at least the vanilla version of central limit theorem not applicable. So that normal distribution approximation is not a good one anymore. And in that sense, maybe it's more like when central limit theorem cannot be used, so that whether it can warn us about something. So a very, I guess, interesting case and case that is not very rare to come across is cases where we are dealing with heavy tail distributions. So this could be distributions, for instance, when your standard deviation is not defined because you, I'm no expert on heavy tail distributions from a mathematical perspective. My intuitive understanding is that these are distributions where unlike, let's say, your typical normal distribution, when you get some standard deviations further from the true mean, then the probability of seeing an example there is exponentially low, basically. And this curve starts going down at a very fast rate once you start getting very far from the mean. But there are some distributions where no matter how far you go from the mean, there's still a good possibility of observing examples. So then the rate of seeing that probability is not sub-legoritmic or something. It's bigger than any kind of threshold that you can put there. A very... Can you give a real world example? Yeah, exactly. I wanted to go there. So I think one of the best examples you see there are events that can kind of form a feedback loop. One of the most common ones is social network phenomena. So let's say this person gets famous on Twitter. He starts gathering his network and then other people start retweeting his tweets. So then this network starts growing. And this is a matter with a lot of forms of also natural popularity in society. So the number of friends, the number of connections, the number of people who see somebody's tweets, for instance, and a lot of other problems in computer networks, even the size of files that people send around over the internet, these are known to be not governed by normal distributions at all, and then these are the phenomena where heavy tail distributions, long tail distributions can be a better approximation. So when you're dealing with these situations, for instance, one instance that happens is that you don't have a well-defined standard deviation anymore because you can see very, very large numbers, this distribution, and you can't exclude or ignore that probability anymore. So once you go there, then central limit theorem is not your friend. I mean, the vanilla form of central limit theorem is not your friend anymore. I know there are approximations to those functions and more complicated versions of central limit theorem that try to be better approximations of the resulting distribution in some cases, but I'm really out of my depth when things go there. What I'm cautious about is to know if it's likely that I'd be dealing with those kind of scenarios and then just be careful not to apply the wrong tool or not to draw a conclusion which are based on the assumptions of, for instance, having a finite standard deviation in your distribution. But for sure, these are cases which come across more often than we like to admit, and I think there are famous cases in the financial domain, famous cases of people making the basically central limit theorem approximations or a way of reasoning about their data that have resulted in disasters. Yeah, there, for example, is quite common in options. So in financial markets, think about option as kind of like insurance, right? So you say, okay, I want to have insurance against, I don't know, the price of Bitcoin going too high or the price of Tesla stock going too low. And then you can buy kind of like insurance, which is called options. And then there were a lot of newcomers, new traders, they feel that, oh, you know, selling options seems to be really good because, you know, you can sell a pretty good price for the insurance that you are selling. You can sell those, right? And the thing is that you can just keep selling it and keep collecting money, right? So that a lot of people think this is kind of like cash printing machine. But then that is just because they made the assumption that the distribution is a normal distribution. While in reality, like you mentioned, the price of financial products sometimes can go into a feedback loop or sometimes can go into a very rapid change pattern that the probability of going really far out from its current price. So if you look at the price of a stock, right, if today is at X, then tomorrow is less likely to be X minus, say, two Y compared to X minus one Y. So the further away the price change, the less likely. However, if you go towards the extreme, it is non-zero and actually the probability of those happening might even be larger than some of the intermediate levels just because how the feedback loop works in the financial market. So yeah, so don't sell options if you see a get rich quick course. That's a very nice example because I think it also touches upon, a little bit touches upon the independence assumption among the random variables. I'm sure the prices and the transactions that result in those prices are not fully independent. It's likely to assume they are correlated in some way. And one small point there is that if these correlations were linear, so if all these random variables, all these measurements that we were gathering were linear in some way, then still central limit theorem would be applicable. So you might shift the mean or you might be able to form some kind of additive computations that give you the right result. But an interesting point is that when you have these network structures, then you also lose that linearity in the feedback. So you're more likely to deal with feedback loops that are non-linear. So for instance, they have some kind of exponential nature. And that's again, one of the cases where you should be very careful about applying central limit theorem, again, at least in this vanilla form for sure. Okay, so we talk about the implication of the shape of the distribution. We talk about the implication of the does it make sense if it has killed? What happens when there is outliers? What about the sampling sizes? Because we take the sampling and then there are some sample sizes and then you approach. Does something play there? Is there anything we need to keep in mind when we use central limit theorem? Yeah, of course. I mean, you've probably seen it when you were reading the theorem itself, but I think it's one of the key points to understand and to have in mind about the central limit theorem. There's something very intuitive there. So when you think about it, okay, if I draw one sample, if I draw a thousand samples, then it's more likely that thousand samples get closer to mean or the true value that I like to observe. And central limit theorem basically, or lower, for instance, lower large numbers on the very extreme and the infinite and the limits are talking about things converging to the true mean. So central limit theorem also allows us to talk about intermediate values of this number of samples or n that you're dealing with. And again, one beautiful thing there is that it quantifies this notion of I will have more certainty with more samples and basically shows us the variability in the sampling distribution decreases in reverse proportion to your number of samples. So basically if the variability is sigma two for the original distribution of the data, then the sampling distribution's variability is going to be sigma two divided by n when n is the number of samples or equivalent, you can say a standard deviation grows in reverse proportion to square root of n. And that's the very key point when we want to talk about central limit theorem. Again, having that part is what allows us to talk about, for instance, 95% confidence intervals because once we know, okay, again, there have to be assumptions made there about the true standard deviation of the data, which is most of the time another unobserved variable. There have to be assumptions made about the real standard deviation of the original distribution. And once you have that, you can talk about the standard deviation of the sampling distribution. So how does it come into play? You've done one measurement about some kind of data, let's say the benefits, or let's say the blood pressure of people in the treatment group or the control group, or maybe easily you can start talking about the comparison or the relative difference between these two, which is most of the, for instance, A-B test experiments measurements. And what this theorem about the variability of the sampling distribution gives us is now that we can talk about how likely is it that we are this far from the true mean, that we are one standard deviation or two standard deviations from the true mean when we did these measurements. And that's the place where having more examples, having more samples is directly increasing our certainty. So if we want to be sure that we are within a 95% confidence interval, we plug in the standard deviation that we observed in the samples that we have, we plug in the number of samples that we have in the formula, and we get an estimate about the 95% confidence interval. The more samples that we have, the smaller this interval becomes. And that is maybe a good point to also mention one tricky thing about dealing with uncertainty in this way is that the more samples that you have, you can be certain about smaller changes. And that's the trap that people can easily fall into. So that's why a lot of good practices about deciding prior to an experiment on an effect size that you want to observe, a number of samples that you need to observe that effect size become important, because especially with the scale of the internet and many companies having access to hundreds of millions of users, you can always, or it is likely that you can get enough samples to make things look significant, especially if you're then going into the realm of p-values and things like that. So it is a key point to have in mind the number of samples there. It is a key point to have in mind the trade-off that you have between certainty and the number of observations that you get. And this trade-off can become, of course, pretty real in the business world, or even in the real world, in the medical world, for instance. So how many people are you going to expose to this treatment, whether in your mobile app or in your medical experiment? How much certainty will you gain with this number of people? And then always you're weighing it about some potential downsides of your experiment. In the medical case, it is quite obvious. You don't want to hurt people's health. And in the business case, it's also very direct sometimes. If you have a risky experiment, you don't want to expose too many users to it. What if you hurt their experience of your product very bad, your reputation very bad, or just directly starting to lose revenue? So that's where I think central liminal theorem comes into play and comes into play very fundamentally in the way we think about certainty, uncertainty, value, and cost. Yeah, absolutely. And there a lot of times also when you are in a, for example, in a business setting, then another thing you need to think about is that it's relatively easy at the beginning of an experiment or at least the beginning gathering data path, because when you are at that phase, uncertainty is reduced relatively quickly. But then as you mentioned, as the number, the sample size goes higher and higher, it's reverse proportion to the kind of, for example, the confidence interval you want to reduce. And that just means that the more data you gather, the more new data you need in order to reduce uncertainty by the same amount. That just means that at a certain point, you have to say, this is enough and we go from here. But then how to decide that point, how long is that point? Usually, of course, in theory, the best practice has always been, okay, we decide on that number and then we, once at that point, it's a cutoff, right? So a lot of times this kind of like the best practice people try to follow. But in reality, usually not in a black or white situation. You're usually in a situation where it's too, how do you say that? It's not good enough for you to say, yes, we can make a decision, but it's also not bad enough for you to say, no, we shouldn't think about this, right? It's usually in between. So then at that point, a lot of times you need to look at the number of samples you already have and then you need to say, okay, if we want to gather more, how much? And if we want to reduce the uncertainty by X amount, then how many more samples that we need? So this is another kind of like a decision point you might come across doing data science in businesses. Absolutely. And I think there, now that we're talking about experimentation, for instance, I think interesting practice is to also think about experiments, potentially in iterations or at least in different steps. So you might look into a direction with some level of certainty and start seeing some signals and you might form another hypothesis once you have that, even if you don't have enough data, and then you can design a new experiment to specifically look into that and then prepare together enough data to specifically look for a change in that direction where now looks promising but you're not 100% or you're not certain enough about the data that you have at hand. So as long as that hypothesis is well defined and then can always start a new experiment and hopefully now you're in a better position than you were initially. So your uncertainty has reduced by some amount of these based on the initial experiments that you ran. And that's also the case where for effects which might come up unexpectedly. So you might not have expected to see a big difference that part of the business or in that specific metric, you run an experiment, you do observe it. It's not the best practice to start deciding on that effect based on that original experiment that you did not hypothesize about. But this could be a really nice trigger to form a new hypothesis and to design a specific experiment to learn about that new measure. That's a lot. We talk about central limit theorem. Just to recap on my side, the law of large number says that if you keep repeating a process enough times, then you will start seeing the mean measurement of that process turn to converge towards its true mean. So it's true. It's population mean. Let's say that way. And then because of that, when you are repeating the process again and again, and you are collecting different samples and then you collect one sample and then you collect another sample and you collect another sample, the distribution of the mean of those samples, they themselves turn to form a normal distribution. So that is the central limit theorem. That is my current mental image. And there are certain limitations on that. For example, how much you can assume that each process is independent in real world. And also the type of distribution, especially when you have more fat-held distribution, then you really need to be aware of that when you try to apply things that are actually built on top of central limit theorem. So that's my takeaway. Yeah, I would like to add maybe just some practical tip about it. I think we talked about it's one of the most fundamental theorems that is used today, especially considering how widely used experimentation, for instance, has become. But something that I noticed is that it might be less intuitive to think about it when we're doing calculations, aggregations outside of experiments. And that's why I like this notion of thinking about uncertainty in any kind of measurement, maybe thinking about processes as what could be any kind of natural process. So we talked about this, I guess, previously as well. But whenever we're doing some kind of summarization of a population, I would think it would be really beneficial if we have the central limit theorem in mind. And just have in mind that we are drawing conclusions based on a sample of some total population of things, some of which might not have been even generated at the time we're doing the measurements. And having that view in mind, I think might be beneficial, might be helpful in having in mind that whenever we're doing a summarization like that, we are dealing with uncertainty. And of course, central limit theorem is one of the most useful tools, most broadly applicable tools to start measuring that uncertainty and quantifying that. That's basically the root of the reason, or could be thought about as one explanation for the reason that if you gather 10 people and measure their performance and average it, you have less certainty in that information than when you gather 200 people, or much less than when you gather a thousand people. And I think that could be a nice mental model to have in the back of your head whenever we're doing things like that. All right, man. This has been fun. Then I guess that's the end of this episode. Thank you for listening and see you next time. Just one last thing before you go. If you are not a data scientist yet, but want to become one, you should really attend our webinar. We will demystify the transition into data science. We'll show you the most effective way to build your skills. And we will advise you on the four possible options you can take to go from where you are to landing a data science job in as little as nine months. Find out more at nds.show forward slash webinar. That is nds.show forward slash webinar. All right. That's the end of this episode. Have a nice day."}, "podcast_summary": "In this episode of Naked Data Science, they discuss the central limit theorem and its implications. The hosts explain that the theorem builds on the law of large numbers, which states that if you sum up a bunch of independent random variables, their mean will approach the population mean. The central limit theorem takes this a step further and shows that the distribution of sample means will approximate a normal distribution. They discuss the applicability of the theorem to various situations and the importance of sample size in reducing uncertainty. They also mention the limitations of the theorem, such as its inability to account for heavy-tailed distributions. Overall, they provide a thorough explanation of the central limit theorem and its practical implications.", "podcast_guest": {"name": "Central Limit Theorem", "summary": "In probability theory, the central limit theorem (CLT) establishes that, in many situations, for independent and identically distributed random variables, the sampling distribution of the standardized sample mean tends towards the standard normal distribution even if the original variables themselves are not normally distributed.\nThe theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.\nThis theorem has seen many changes during the formal development of probability theory. Previous versions of the theorem date back to 1811, but in its modern general form, this fundamental result in probability theory was precisely stated as late as 1920, thereby serving as a bridge between classical and modern probability theory.\nAn elementary form of the theorem states the following. Let \n  \n    \n      \n        \n          X\n          \n            1\n          \n        \n        ,\n        \n          X\n          \n            2\n          \n        \n        ,\n        \u2026\n        ,\n        \n          X\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle X_{1},X_{2},\\dots ,X_{n}}\n   denote a random sample of \n  \n    \n      \n        n\n      \n    \n    {\\displaystyle n}\n   independent observations from a population with overall expected value (average) \n  \n    \n      \n        \u03bc\n      \n    \n    {\\displaystyle \\mu }\n   and finite variance, and let \n  \n    \n      \n        \n          \n            \n              \n                X\n                \u00af\n              \n            \n          \n          \n            n\n          \n        \n      \n    \n    {\\displaystyle {\\bar {X}}_{n}}\n  denote the sample mean of that sample (which is itself a random variable). Then the limit as \n  \n    \n      \n        n\n        \u2192\n        \u221e\n      \n    \n    {\\displaystyle n\\to \\infty }\n   of the distribution of \n  \n    \n      \n        \n          \n            \n              \n                \n                  \n                    \n                      X\n                      \u00af\n                    \n                  \n                \n                \n                  n\n                \n              \n              \u2212\n              \u03bc\n            \n            \n              \u03c3\n              \n                \n                  \n                    \n                      \n                        X\n                        \u00af\n                      \n                    \n                  \n                  \n                    n\n                  \n                \n              \n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle {\\frac {{\\bar {X}}_{n}-\\mu }{\\sigma _{{\\bar {X}}_{n}}}},}\n   where \n  \n    \n      \n        \n          \u03c3\n          \n            \n              \n                \n                  \n                    X\n                    \u00af\n                  \n                \n              \n              \n                n\n              \n            \n          \n        \n        =\n        \n          \n            \u03c3\n            \n              n\n            \n          \n        \n        ,\n      \n    \n    {\\displaystyle \\sigma _{{\\bar {X}}_{n}}={\\frac {\\sigma }{\\sqrt {n}}},}\n   is the standard normal distribution.In other words, suppose that a large sample of observations is obtained, each observation being randomly produced in a way that does not depend on the values of the other observations, and that the average (arithmetic mean) of the observed values is computed. If this procedure is performed many times, resulting in a collection of observed averages, the central limit theorem says that if the sample size was large enough, the probability distribution of these averages will closely approximate a normal distribution.\nThe central limit theorem has several variants. In its common form, the random variables must be independent and identically distributed (i.i.d.). This requirement can be weakened; convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations if they comply with certain conditions.\nThe earliest version of this theorem, that the normal distribution may be used as an approximation to the binomial distribution, is the de Moivre\u2013Laplace theorem."}, "podcast_highlights": "In this episode of the podcast, Naked Data Science, the hosts discuss the central limit theorem. They explain the concept of the law of large numbers, which states that the more times you run a process, the closer the average of that process will get to the population mean. They then discuss the central limit theorem, which builds on the law of large numbers and states that the sum of random variables from different processes, as long as they are independent, will tend to follow a normal distribution. They also discuss the application of the central limit theorem, such as in A/B testing and experimental design. The hosts caution against the assumptions and limitations of the central limit theorem, such as the requirement for independent and identical copies of a random variable, and the limitations of applying the theorem to heavy-tailed distributions or distributions with outliers. They also discuss the impact of sample size on the accuracy of the theorem and the concept of uncertainty in data analysis. Overall, the episode provides a informative overview of the central limit theorem and its practical application in data science."}